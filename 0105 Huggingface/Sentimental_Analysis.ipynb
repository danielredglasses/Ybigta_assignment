{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lIwMS_o9eGWr"
      },
      "outputs": [],
      "source": [
        "!pip3 install datasets\n",
        "!pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZeZxd_W_kMvG"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJKI7thPc7mK",
        "outputId": "fa076a08-cfee-4b75-b68b-5ec444a8bb28"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"sepidmnorozy/Korean_sentiment\")\n",
        "\n",
        "x_train, y_train = dataset['train']['text'], dataset['train']['label']\n",
        "x_test, y_test = dataset['validation']['text'], dataset['validation']['label']\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "train_dataset = tokenizer(x_train, padding = True, truncation = True, return_tensors = \"pt\")\n",
        "train_dataset = TensorDataset(train_dataset['input_ids'], train_dataset['attention_mask'], torch.tensor(y_train))\n",
        "\n",
        "test_dataset = tokenizer(x_test, padding = True, truncation = True, return_tensors = \"pt\")\n",
        "test_dataset = TensorDataset(test_dataset['input_ids'], test_dataset['attention_mask'], torch.tensor(y_test))\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = 100, shuffle = True, num_workers = 4)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 100, shuffle = False, num_workers = 4)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels = 2).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_JI2lHL8i5Dp"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    train_loss, train_acc, total_samples = 0, 0, 0\n",
        "\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "        input_ids, attention_mask, labels = data\n",
        "        input_ids, attention_mask, labels = input_ids.to(DEVICE), attention_mask.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask = attention_mask, labels = labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        preds = torch.argmax(outputs.logits, dim = 1)\n",
        "        train_acc += torch.sum(preds == labels).item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    train_loss = train_loss / len(train_loader)\n",
        "    train_acc = train_acc / total_samples\n",
        "\n",
        "    return train_loss, train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKFqyeVXh0in",
        "outputId": "08fecfeb-25e8-4087-8e6f-07af4a38e519"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            " 33%|███▎      | 1/3 [13:17<26:34, 797.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: train_loss 0.47478451935781374, train_acc 0.7657777777777778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 2/3 [26:32<13:16, 796.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: train_loss 0.3480254902193944, train_acc 0.8452777777777778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [39:48<00:00, 796.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: train_loss 0.2840343111505111, train_acc 0.8778333333333334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 2e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer)\n",
        "    print(f'Epoch {epoch+1}: train_loss {train_loss}, train_acc {train_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye2OFJ6XFhEC",
        "outputId": "e4b07d9e-b0f3-45dc-b22a-79fb3b5647e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "input = \"실제 역사를 거의 그대로 다루다보니 보면서 화가 나긴 하지만 영화적으로 흥미롭게 구성을 잘해서 몰입도가 높아요. 몇 번을 봐도 볼 때마다 새롭게 보이는 면이 있구요. 의미와 재미를 다 잡은 작품이에요!\"\n",
        "input = tokenizer(input, padding = True, truncation = True, return_tensors = \"pt\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    input_ids = input['input_ids'].to(DEVICE)\n",
        "    outputs = model(input_ids)\n",
        "    preds = torch.argmax(outputs.logits, dim = 1)\n",
        "    print(preds)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
